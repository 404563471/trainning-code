---
title: "algorithm"
author: "mason-YHY"
date: "2020/7/3"
output: html_document
---

###Prepare and keep data set.



```{r, message=FALSE, warning=FALSE}

###lets prepare and keep data in the proper format



feauter1<-full[1:891, c("Pclass", "title","Sex","Embarked","FamilySized","ticket.size")]

response <- as.factor(train$Survived)

feauter1$Survived=as.factor(train$Survived)





###For Cross validation purpose will keep 20% of data aside from my orginal train set

##This is just to check how well my data works for unseen data

set.seed(500)

ind=createDataPartition(feauter1$Survived,times=1,p=0.8,list=FALSE)

train_val=feauter1[ind,]

test_val=feauter1[-ind,]



```





```{r, message=FALSE, warning=FALSE}



####check the proprtion of Survival rate in orginal training data, current traing and testing data

round(prop.table(table(train$Survived)*100),digits = 1)

round(prop.table(table(train_val$Survived)*100),digits = 1)

round(prop.table(table(test_val$Survived)*100),digits = 1)





```







                                      

**JAMES**   - Let’s do training with algorithms.                                                                                                                  





**MARK**    - After training with algorithms,  what’s next ?                                                                                                                                                                   

**JAMES**   - We have to validate our trained algorithms with test data set.                                                                                





**MARK**    - How do we measure we our algorithms performance?                                                   

**JAMES**   - With [goodness of fit](https://en.wikipedia.org/wiki/Goodness_of_fit),  lets go with Confusion Matrix for validation.



                                                                                                                                                         

                                                                                                                                                         

                                                                                                                                                         

                                                                                                                                                         



**Note - Go through each tab for different algorithms**



### Predictive Analysis and Cross Validation {.tabset}



#### Decison tree {-}



```{r, message=FALSE, warning=FALSE}



##Random forest is for more better than Single tree however single tree is very easy to use and illustrate

set.seed(1234)

Model_DT=rpart(Survived~.,data=train_val,method="class")





rpart.plot(Model_DT,extra =  3,fallen.leaves = T)



###Surprise, Check out the plot,  our Single tree model is using only Title, Pclass and Ticket.size and vomited rest

###Lets Predict train data and check the accuracy of single tree

```





```{r, message=FALSE, warning=FALSE}

PRE_TDT=predict(Model_DT,data=train_val,type="class")

confusionMatrix(PRE_TDT,train_val$Survived)



#####Accuracy is 0.8375

####Not at all bad using Single tree and just 3 feauters



##There is chance of overfitting in Single tree, So I will go for cross validation using '10 fold techinque'



set.seed(1234)

cv.10 <- createMultiFolds(train_val$Survived, k = 10, times = 10)



# Control

ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10,

                       index = cv.10)



                     



train_val <- as.data.frame(train_val)



##Train the data

Model_CDT <- train(x = train_val[,-7], y = train_val[,7], method = "rpart", tuneLength = 30,

                   trControl = ctrl)



##Check the accurcay

##Accurcay using 10 fold cross validation of Single tree is 0.8139 

##Seems Overfitted earlier using Single tree, there our accurcay rate is 0.83



# check the variable imporatnce, is it the same as in Single tree?

rpart.plot(Model_CDT$finalModel,extra =  3,fallen.leaves = T)



##Yes, there is no change in the imporatnce of variable









###Lets cross validate the accurcay using data that kept aside for testing purpose

PRE_VDTS=predict(Model_CDT$finalModel,newdata=test_val,type="class")

confusionMatrix(PRE_VDTS,test_val$Survived)



###There it is, How exactly our train data and test data matches in accuracy (0.8192)





col_names <- names(train_val)



train_val[col_names] <- lapply(train_val[col_names] , factor)

test_val[col_names] <- lapply(test_val[col_names] , factor)



```





#### Random Forest {-}



```{r, message=FALSE, warning=FALSE}



set.seed(1234)





rf.1 <- randomForest(x = train_val[,-7],y=train_val[,7], importance = TRUE, ntree = 1000)

rf.1

varImpPlot(rf.1)







####Random Forest accurcay rate is 82.91 which is 1% better than the decison  tree

####Lets remove 2 redaundant varibles and do the modeling again

train_val1=train_val[,-4:-5]

test_val1=test_val[,-4:-5]





set.seed(1234)

rf.2 <- randomForest(x = train_val1[,-5],y=train_val1[,5], importance = TRUE, ntree = 1000)

rf.2

varImpPlot(rf.2)



###Can see the Magic now, increase in accuracy by just removing 2 varibles, accuracy now is 84.03 



##Even though random forest is so power full we accept the model only after cross validation





set.seed(2348)

cv10_1 <- createMultiFolds(train_val1[,5], k = 10, times = 10)



# Set up caret's trainControl object per above.

ctrl_1 <- trainControl(method = "repeatedcv", number = 10, repeats = 10,

                      index = cv10_1)







set.seed(1234)

rf.5<- train(x = train_val1[,-5], y = train_val1[,5], method = "rf", tuneLength = 3,

              ntree = 1000, trControl =ctrl_1)



rf.5



##Cross validation give us the accurcay rate of .8393



###Lets Predict the test data 



pr.rf=predict(rf.5,newdata = test_val1)



confusionMatrix(pr.rf,test_val1$Survived)



####accuracy rate is 0.8192, lower than what we have expected  



```



#### lasso-ridge regression {-}





```{r, message=FALSE, warning=FALSE}



train_val <- train_val %>%

  mutate(Survived = case_when(Survived==1 ~ "Yes", 

                              Survived==0 ~ "No"))







train_val<- as.data.frame(train_val)

train_val$title<-as.factor(train_val$title)

train_val$Embarked<-as.factor(train_val$Embarked)

train_val$ticket.size<-as.factor(train_val$ticket.size)



table(train_val$Survived)



test_val<- as.data.frame(test_val)

test_val$title<-as.factor(test_val$title)

test_val$Embarked<-as.factor(test_val$Embarked)

test_val$ticket.size<-as.factor(test_val$ticket.size)

test_val$Survived<-as.factor(test_val$Survived)







train.male = subset(train_val, train_val$Sex == "male")

train.female = subset(train_val, train_val$Sex == "female")

test.male = subset(test_val, test_val$Sex == "male")

test.female = subset(test_val, test_val$Sex == "female")





train.male$Sex = NULL



train.male$title = droplevels(train.male$title)



train.female$Sex = NULL

train.female$title = droplevels(train.female$title)



test.male$Sex = NULL





test.male$title = droplevels(test.male$title)





test.female$Sex = NULL

test.female$title = droplevels(test.female$title)



set.seed(101) 

train_ind <- sample.split(train.male$Survived, SplitRatio = .75)





# MALE



## set the seed to make your partition reproductible





cv.train.m <- train.male[train_ind, ]

cv.test.m  <- train.male[-train_ind, ]



# FEMALE

set.seed(100)



## set the seed to make your partition reproductible

set.seed(123)

train_ind <- sample.split(train.female$Survived, SplitRatio = .75)



cv.train.f <- train.male[train_ind, ]

cv.test.f  <- train.male[-train_ind, ]





x.m = data.matrix(cv.train.m[,1:5])

y.m = cv.train.m$Survived





set.seed(356)

# 10 fold cross validation

cvfit.m.ridge = cv.glmnet(x.m, y.m, 

                  family = "binomial", 

                  alpha = 0,

                  type.measure = "class")



cvfit.m.lasso = cv.glmnet(x.m, y.m, 

                  family = "binomial", 

                  alpha = 1,

                  type.measure = "class")

par(mfrow=c(1,2))

plot(cvfit.m.ridge, main = "Ridge")

plot(cvfit.m.lasso, main = "Lasso")



coef(cvfit.m.ridge, s = "lambda.min")



# Prediction on training set

PredTrain.M = predict(cvfit.m.ridge, newx=x.m, type="class")





table(cv.train.m$Survived, PredTrain.M, cv.train.m$title)



# Prediction on validation set

PredTest.M = predict(cvfit.m.ridge, newx=data.matrix(cv.test.m[,1:5]), type="class")

table(cv.test.m$Survived, PredTest.M, cv.test.m$title)





# Prediction on test set

PredTest.M = predict(cvfit.m.ridge, newx=data.matrix(test.male[,1:5]), type="class")

table(PredTest.M, test.male$title)





#female

x.f = data.matrix(cv.train.f[,1:5])

y.f = cv.train.f$Survived



set.seed(356)

cvfit.f.ridge = cv.glmnet(x.f, y.f, 

                  family = "binomial", 

                  alpha = 0,

                  type.measure = "class")

cvfit.f.lasso = cv.glmnet(x.f, y.f, 

                  family = "binomial", 

                  alpha = 1,

                  type.measure = "class")

par(mfrow=c(1,2))

plot(cvfit.f.ridge, main = "Ridge")

plot(cvfit.f.lasso, main = "Lasso")



coef(cvfit.f.ridge, s = "lambda.min")



# Ridge Model

# Prediction on training set

PredTrain.F = predict(cvfit.f.ridge, newx=x.f, type="class")

table(cv.train.f$Survived, PredTrain.F, cv.train.f$title)



confusionMatrix(cv.train.f$Survived, PredTrain.F)





# Prediction on validation set

PredTest.F = predict(cvfit.f.ridge, newx=data.matrix(cv.test.f[,1:5]), type="class")

table(cv.test.f$Survived, PredTest.F, cv.test.f$title)



confusionMatrix(cv.test.f$Survived, PredTest.F)





# Ridge Model

# Prediction on training set

PredTrain.F = predict(cvfit.f.lasso, newx=x.f, type="class")

table(cv.train.f$Survived, PredTrain.F, cv.train.f$title)



confusionMatrix(cv.train.f$Survived, PredTrain.F)



# Prediction on validation set

PredTest.F = predict(cvfit.f.lasso, newx=data.matrix(cv.test.f[,1:5]), type="class")

table(cv.test.f$Survived, PredTest.F, cv.test.f$title)



confusionMatrix(cv.test.f$Survived, PredTest.F)





# Prediction on test set

PredTest.F = predict(cvfit.f.ridge, newx=data.matrix(test.female[,1:5]), type="class")

table(PredTest.F, test.female$title)





MySubmission.F<-cbind(cv.train.m$Survived, PredTrain.M)

MySubmission.M<-cbind(cv.train.f$Survived, PredTrain.F)





MySubmission<-rbind(MySubmission.M,MySubmission.F)



colnames(MySubmission) <- c('Actual_Survived', 'predict')

MySubmission<- as.data.frame(MySubmission)



confusionMatrix(MySubmission$Actual_Survived, MySubmission$predict)



```









#### Support Vector Machine - Linear Support vector Machine {-}

```{r, message=FALSE, warning=FALSE}



###Before going to model lets tune the cost Parameter



set.seed(1274)

liner.tune=tune.svm(Survived~.,data=train_val1,kernel="linear",cost=c(0.01,0.1,0.2,0.5,0.7,1,2,3,5,10,15,20,50,100))



liner.tune



###best perforamnce when cost=3 and accuracy rate is 82.7





###Lets get a best.liner model  

best.linear=liner.tune$best.model



##Predict Survival rate using test data



best.test=predict(best.linear,newdata=test_val1,type="class")

confusionMatrix(best.test,test_val1$Survived)



###Linear model accuracy is 0.8136

```



#### XGBoost {-}

```{r, message=FALSE, warning=FALSE}





library(xgboost)

library(MLmetrics)



train <- read_csv('../input/train.csv')

test  <- read_csv('../input/test.csv')



train$set <- "train"

test$set  <- "test"

test$Survived <- NA

full <- rbind(train, test)



full <- full %>%

    mutate(

      Age = ifelse(is.na(Age), mean(full$Age, na.rm=TRUE), Age),

      `Age Group` = case_when(Age < 13 ~ "Age.0012", 

                                 Age >= 13 & Age < 18 ~ "Age.1317",

                                 Age >= 18 & Age < 60 ~ "Age.1859",

                                 Age >= 60 ~ "Age.60Ov"))

                                 

full$Embarked <- replace(full$Embarked, which(is.na(full$Embarked)), 'S')





full <- full %>%

  mutate(Title = as.factor(str_sub(Name, str_locate(Name, ",")[, 1] + 2, str_locate(Name, "\\.")[, 1]- 1)))







full <- full %>%

  mutate(`Family Size`  = as.numeric(SibSp) + as.numeric(Parch) + 1,

         `Family Group` = case_when(

           `Family Size`==1 ~ "single",

           `Family Size`>1 & `Family Size` <=3 ~ "small",

           `Family Size`>= 4 ~ "large"

         ))

         

full <- full %>%

  mutate(Survived = case_when(Survived==1 ~ "Yes", 

                              Survived==0 ~ "No"))





full_2 <- full %>% 

  select(-Name, -Ticket, -Cabin, -set) %>%

  mutate(

    Survived = ifelse(Survived=="Yes", 1, 0)

  ) %>% 

  rename(AgeGroup=`Age Group`, FamilySize=`Family Size`, FamilyGroup=`Family Group`)





# OHE

ohe_cols <- c("Pclass", "Sex", "Embarked", "Title", "AgeGroup", "FamilyGroup")

num_cols <- setdiff(colnames(full_2), ohe_cols)



full_final <- subset(full_2, select=num_cols)



for(var in ohe_cols) {

  values <- unique(full_2[[var]])

  for(j in 1:length(values)) {

    full_final[[paste0(var,"_",values[j])]] <- (full_2[[var]] == values[j]) * 1

  }

}





submission <- TRUE



data_train <- full_final %>%

  filter(!is.na(Survived)) 



data_test  <- full_final %>% 

  filter(is.na(Survived))



set.seed(777)

ids <- sample(nrow(data_train))



# create folds for cv

n_folds <- ifelse(submission, 1, 5)



score <- data.table()

result <- data.table()







for(i in 1:n_folds) {

  

  if(submission) {

    x_train <- data_train %>% select(-PassengerId, -Survived)

    x_test  <- data_test %>% select(-PassengerId, -Survived)

    y_train <- data_train$Survived

    

  } else {

    train.ids <- ids[-seq(i, length(ids), by=n_folds)]

    test.ids  <- ids[seq(i, length(ids), by=n_folds)]

    

    x_train <- data_train %>% select(-PassengerId, -Survived)

    x_train <- x_train[train.ids,]

    

    x_test  <- data_train %>% select(-PassengerId, -Survived)

    x_test  <- x_test[test.ids,]

    

    y_train <- data_train$Survived[train.ids]

    y_test  <- data_train$Survived[test.ids]

  }

  

  x_train <- apply(x_train, 2, as.numeric)

  x_test <- apply(x_test, 2, as.numeric)

  

  if(submission) {

    nrounds <- 12

    early_stopping_round <- NULL

    dtrain <- xgb.DMatrix(data=as.matrix(x_train), label=y_train)

    dtest <- xgb.DMatrix(data=as.matrix(x_test))

    watchlist <- list(train=dtrain)

  } else {

    nrounds <- 3000

    early_stopping_round <- 100

    dtrain <- xgb.DMatrix(data=as.matrix(x_train), label=y_train)

    dtest <- xgb.DMatrix(data=as.matrix(x_test), label=y_test)

    watchlist <- list(train=dtrain, test=dtest)

  }

  

  params <- list("eta"=0.01,

                 "max_depth"=8,

                 "colsample_bytree"=0.3528,

                 "min_child_weight"=1,

                 "subsample"=1,

                 "objective"="reg:logistic",

                 "eval_metric"="auc")

  

  model_xgb <- xgb.train(params=params,

                         data=dtrain,

                         maximize=TRUE,

                         nrounds=nrounds,

                         watchlist=watchlist,

                         early_stopping_round=early_stopping_round,

                         print_every_n=2)

  

  pred <- predict(model_xgb, dtest)

  

  if(submission) {

    result <- cbind(data_test %>% select(PassengerId), Survived=round(pred, 0))

  } else {

    score <- rbind(score, 

                   data.frame(accuracy=Accuracy(round(pred, 0), y_test), best_iteration=model_xgb$best_iteration))

    temp   <- cbind(data_train[test.ids,], pred=pred)

    result <- rbind(result, temp)

  }

}





head(result)



```

















#### bRadial Support vector Machine {-}



```{r, message=FALSE, warning=FALSE}





######Lets go to non liner SVM, Radial Kerenl

set.seed(1274)



rd.poly=tune.svm(Survived~.,data=train_val1,kernel="radial",gamma=seq(0.1,5))



summary(rd.poly)

best.rd=rd.poly$best.model



###Non Linear Kerenel giving us a better accuray 



##Lets Predict test data

pre.rd=predict(best.rd,newdata = test_val1)



confusionMatrix(pre.rd,test_val1$Survived)



####Accurcay of test data using Non Liner model is 0.81

####it could be due to we are using smaller set of sample for testing data

```



#### Logistic Regression {-}



```{r, message=FALSE, warning=FALSE}



contrasts(train_val1$Sex)

contrasts(train_val1$Pclass)



##The above shows how the varible coded among themself



##Lets run Logistic regression model

log.mod <- glm(Survived ~ ., family = binomial(link=logit), 

               data = train_val1)

###Check the summary

summary(log.mod)

confint(log.mod)



###Predict train data

train.probs <- predict(log.mod, data=train_val1,type =  "response")

table(train_val1$Survived,train.probs>0.5)



(395+204)/(395+204+70+45)



###Logistic regression predicted train data with accuracy rate of 0.83 



test.probs <- predict(log.mod, newdata=test_val1,type =  "response")

table(test_val1$Survived,test.probs>0.5)



(97+47)/(97+12+21+47)



###Accuracy rate of test data is 0.8135



```

                                                                                                                                                              

                                                                                                                                                              

## Evaluate Machine Learning Algorithms



                                                                                                                                                              

Accuracy with **Random Forest**                  - 84.03%                                                                                                                                                                

Accuracy with **Dession trees**                  - 83.75%                                                                                                                                                                                                                                                                                                                                  

Accuracy with **Radial Support vector Machine**  - 81.92%                                                                                             

Accuracy with **lasso-ridge regression**         - 81.90%                                                                                                                                                                                                                                                                                                                                  

Accuracy with **Linear Support vector Machine**  - 81.36%                                                                                                                                                                

Accuracy with **Logistic Regression**            - 81.36%                                                                                                                                                                





**MARK**    - Oh Gowd, Random Forest works good with  84.03%  accuracy                                                                                                                                                             

**JAMES**   - Yes!!                                                                                                                                                               

                                                                                                                                                                                                        
